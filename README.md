# APS 2 de Natural Language Processing

## Feito por üßë‚Äçü§ù‚Äçüßë:
- Henrique Martinelli Frezzatti

## Bibliotecas necess√°rias:
Todas as bibliotecas que forem obrigat√≥rias para a execu√ß√£o do programa se encontram no arquivo **requirements.txt** que podem ser instaladas com os comandos abaixo:
```bash
pip install -r requirements.txt
```

## Como rodar e testar? üñ•Ô∏è
Todas as execu√ß√µes e testes necess√°rios j√° foram rodados e podem ser replicados ao executar as c√©lulas do arquivo do Jupyter Notebook `app.ipynb`

---
# Introdu√ß√£o da APS üí°

A APS consiste em desenvolver um sistema de recupera√ß√£o de informa√ß√µes que utiliza embeddings para representar documentos e consultas. O objetivo √© melhorar a precis√£o e relev√¢ncia das buscas em um conjunto de dados, utilizando representa√ß√µes vetoriais em um espa√ßo sem√¢ntico. Este sistema ir√° substituir o m√©todo anterior baseado em bag-of-words, empregando a similaridade do cosseno como m√©trica para avaliar a dist√¢ncia sem√¢ntica entre itens no conjunto de dados.

Os principais passos incluem a gera√ß√£o de embeddings para os documentos e consultas, e a implementa√ß√£o de uma fun√ß√£o de busca que utiliza a similaridade do cosseno para recuperar os documentos mais relevantes. O resultado final √© um sistema de busca que oferece uma representa√ß√£o mais rica e sem√¢ntica dos textos, facilitando a recupera√ß√£o de informa√ß√µes de maneira eficiente.

## Banco de dados üìÇ:
O banco de dados para essa APS foi criado com a utiliza√ß√£o da API do _Art Institute of Chicago_ e, a sua utiliza√ß√£o, foi motivada pela necessidade de encontrar artistas, obras e pinturas que remetem a um determinado estilo/palavra determinada pelo usu√°rio. Logo, se o usu√°rio quiser encontrar as obras de arte referentes ao movimento surrealista, ele poderia realizar essa busca e encontrar as artes que mais condizem com o que deseja ver, podendo obter informa√ß√µes extras sobre aquela obra como o artista que a pintou, o ano em que foi pintada e sua descri√ß√£o.

Dessa forma, o sistema n√£o apenas facilita a descoberta de obras, mas tamb√©m promove a aprecia√ß√£o da arte ao oferecer contexto e informa√ß√µes adicionais sobre cada obra.

Para a cria√ß√£o desse banco de dados, foi utilizado o c√≥digo localizado em `db_creation.py` e, o resultado de sua execu√ß√£o, ir√° criar um `.csv` com 10 mil itens dessa API localizado em `art-db.csv`.

# Etapa 1: Gera√ß√£o de Embeddings üóÑÔ∏è

## Processo de Gera√ß√£o de Embeddings
Para a gera√ß√£o dos embeddings, foi utilizado o modelo pretreinado **sBERT** (Sentence-BERT), especificamente a vers√£o "all-MiniLM-L6-v2". Este modelo √© projetado para produzir embeddings de alta qualidade e rapidez, permitindo a representa√ß√£o sem√¢ntica das entradas textuais. Os embeddings gerados para cada obra foram ent√£o utilizados como entrada para um autoencoder, que √© uma rede neural com a seguinte topologia:
- **Encoder:** Camadas lineares que transformam a dimens√£o de entrada em uma dimens√£o reduzida (128).
- **Decoder:** Camadas lineares que reconstr√µem os embeddings a partir da representa√ß√£o reduzida.
Os hiperpar√¢metros utilizados incluem um tamanho de lote de 64 e uma taxa de aprendizado de 0.001.

## Processo de Treinamento
O treinamento do autoencoder foi realizado utilizando a fun√ß√£o de perda **MSE (Mean Squared Error)**, que mede a diferen√ßa entre os embeddings de entrada e suas reconstru√ß√µes. Esta fun√ß√£o de perda √© apropriada neste contexto, pois busca minimizar a diferen√ßa entre as representa√ß√µes originais e as reconstru√≠das, garantindo que os embeddings otimizados preservem as caracter√≠sticas sem√¢nticas das entradas. O treinamento foi executado por 10 √©pocas, onde a cada itera√ß√£o o modelo ajusta seus par√¢metros para melhorar a precis√£o da reconstru√ß√£o.

A equa√ß√£o da fun√ß√£o de perda √© dada por:

\[ \text{Loss} = \frac{1}{n} \sum_{i=1}^{n} (x_i - \hat{x}_i)^2 \]

onde \(x_i\) s√£o os embeddings originais e \(\hat{x}_i\) s√£o as reconstru√ß√µes do autoencoder.

![Modelo do Autoencoder](embeddings_otimizados.png)

# Etapa 2: Visualiza√ß√£o dos Embeddings üìä

Ap√≥s a gera√ß√£o dos embeddings para os itens do dataset, o pr√≥ximo passo √© visualiz√°-los. Para isso, foi utilizado o algoritmo **t-SNE** (t-Distributed Stochastic Neighbor Embedding), que √© mais adequado do que o PCA para proje√ß√£o de espa√ßos de alta dimens√£o em um espa√ßo 2D, preservando melhor a estrutura local dos dados. Com t-SNE, √© poss√≠vel observar como os documentos est√£o organizados e identificar potenciais agrupamentos.

## Proje√ß√£o e Visualiza√ß√£o dos Embeddings
Os embeddings gerados foram projetados para um espa√ßo 2D usando t-SNE. Duas visualiza√ß√µes foram criadas: uma para os embeddings pr√©-treinados e outra para os embeddings otimizados pelo autoencoder. As visualiza√ß√µes est√£o apresentadas nas figuras a seguir:

### Embeddings Originais (Pr√©-treinados)
![Embeddings Originais](embeddings_originais.png)

### Embeddings Otimizados
![Embeddings Otimizados](embeddings_otimizados.png)

### Discuss√£o sobre os Resultados
Na visualiza√ß√£o dos **embeddings originais**, podemos observar uma certa dispers√£o entre as obras de arte, mas tamb√©m h√° √°reas onde documentos semelhantes parecem estar pr√≥ximos, embora n√£o estejam claramente agrupados. Essa dispers√£o pode indicar que os embeddings pr√©-treinados n√£o capturam completamente a sem√¢ntica espec√≠fica do nosso dataset.

Em contrapartida, a visualiza√ß√£o dos **embeddings otimizados** revela uma organiza√ß√£o mais clara e clusters distintos. Esses agrupamentos podem estar relacionados ao conte√∫do das obras, como estilos de arte ou per√≠odos hist√≥ricos. A diferen√ßa na estrutura dos clusters sugere que o autoencoder conseguiu extrair informa√ß√µes mais relevantes, melhorando a representa√ß√£o das obras no espa√ßo vetorial.

# Etapa 3: Teste do Sistema de Busca üîç

Ap√≥s a visualiza√ß√£o dos embeddings, foi desenvolvido um sistema de busca para consultar as obras de arte no dataset. O sistema foi implementado utilizando a mesma l√≥gica do projeto APS-1, mas sem os modelos originais. Logo, as mesmas queries foram feitas. Por√©m, o n√∫mero de resultados para `impressionism` e `surrealism` foram invertidos quando comparados aos obtidos na APS-1. Isso se deve √† forma como os embeddings capturam a sem√¢ntica dos termos e a distribui√ß√£o das obras no espa√ßo vetorial.

No modelo com embeddings, a consulta por `impressionism` gerou 10 resultados, enquanto a consulta por `surrealism` retornou apenas 3 resultados. Essa invers√£o ocorre porque os embeddings s√£o projetados para representar a similaridade sem√¢ntica entre os documentos, e, neste caso, as obras associadas ao termo "impressionism" est√£o mais densamente agrupadas no espa√ßo de embedding. Isso sugere que h√° uma maior quantidade de obras de arte que refletem caracter√≠sticas do impressionismo, tornando-as mais f√°ceis de serem identificadas pelo sistema de busca.

Por outro lado, o termo "surrealism" tem um n√∫mero mais restrito de obras que realmente capturam a ess√™ncia do estilo, resultando em menos correspond√™ncias. Essa diferen√ßa na contagem de resultados pode ser atribu√≠da √† forma como as caracter√≠sticas sem√¢nticas s√£o interpretadas pelos embeddings, que podem n√£o captar a mesma riqueza de contexto ou a variedade de obras associadas ao surrealismo em compara√ß√£o com o impressionismo.

Em contraste, no modelo TF-IDF, a situa√ß√£o era inversa, onde `surrealism` frequentemente retornava mais resultados do que `impressionism`. Isso pode ser explicado pela abordagem de TF-IDF, que se concentra em termos de alta frequ√™ncia e relev√¢ncia, independentemente da estrutura sem√¢ntica. Como resultado, pode haver mais obras que contenham as palavras "surrealism" em seus t√≠tulos ou descri√ß√µes, levando a uma maior contagem de resultados.

## Rubrica e pontos realizados üü¢:
- [X] Cria√ß√£o e configura√ß√£o da API via Flask ou FastAPI
- [X] Escolha da API para gera√ß√£o do banco de dados
- [X] Cria√ß√£o do banco de dados
- [X] Cria√ß√£o do classificador de relev√¢ncia
- [X] Completar o README.md
